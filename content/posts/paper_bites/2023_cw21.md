---
title: "Paper bites of 2023 CW21"
date: 2023-05-23T20:21:15+02:00
draft: false
mathjax: true
---


## Tree of Thoughts: Deliberate Problem Solving with Large Language Models

We all know the trick of "think step by step" when we ask LLM to resolve tricky questions. 

The drawbacks of CoT is that:
- Locally, they do not explore different continuations within a thought process – the branches of the tree. 
- Globally, they do not incorporate any type of **planning**, **lookahead**, or **backtracking** to help evaluate these different options – the kind of heuristic-guided search that seems characteristic of human problem-solving.

ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.

![](/paper_bites/tot_1.png#center)

### How to evaluate intermediate thought

The key problem is how to evaluate the intermediate state(thought)? The author also uses the LM to reason about the interediate states to give a score for each state. An simple [implementation](https://github.com/kyegomez/tree-of-thoughts/blob/main/tree_of_thoughts/openaiModels.py) which ask the LM to strictly give a scalar value between 0 and 1:
```py
def evaluate_states(self, states, inital_prompt):

    if self.evaluation_strategy == 'value':
        state_values = {}
        for state in states:
            if (type(state) == str):
                state_text = state
            else:
                state_text = '\n'.join(state)
            print("We receive a state of type", type(state), "For state: ", state, "\n\n")
            prompt = f"Given the current state of reasoning: '{state_text}', evaluate its value as a float between 0 and 1, become very pessimistic think of potential adverse risks on the probability of this state of reasoning achieveing {inital_prompt} and DO NOT RESPOND WITH ANYTHING ELSE: OTHER THAN AN FLOAT"
            
            response = self.openai_api_call_handler(prompt, 10, 1)
            try:
                value_text = self.openai_choice2text_handler(response.choices[0])
                value = float(value_text)
                print(f"value: {value}")
            except ValueError:
                value = 0  # Assign a default value if the conversion fails
            state_values[state] = value
        return state_values

    elif self.evaluation_strategy == 'vote':
        states_text = '\n'.join([' '.join(state) for state in states])

        prompt = f"Given the following states of reasoning, vote for the best state utilizing an scalar value 1-10:\n{states_text}\n\nVote, on the probability of this state of reasoning achieveing {inital_prompt} and become very pessimistic very NOTHING ELSE"

        response = self.openai_api_call_handler(prompt, 50, 1)

        print(f'state response: {response}')

        best_state_text = self.openai_choice2text_handler(response.choices[0])

        print(f"Best state text: {best_state_text}")

        best_state = tuple(best_state_text.split())

        print(f'best_state: {best_state}')

        return {state: 1 if state == best_state else 0 for state in states}

    else:
        raise ValueError("Invalid evaluation strategy. Choose 'value' or 'vote'.")
```

### Optimal Path search algorithm

Within the ToT framework, the author explored two relatively simple search algorithms: 
- Breadth-first search (BFS) maintains a set of the b most promising states per step. This is used for Game of 24 and Creative Writing where the tree depth is limit (T ≤ 3), and initial thought steps can be evaluated and pruned to a small set (b ≤ 5).
- Depth-first search (DFS) explores the most promising state first, until the final output is reached (t > T), or the state evaluator deems it impossible to solve the problem from the current $s(V(p_{\theta}, {s})(s) ≤ v_{th}$ for a value threshold $v_{th}$). In the latter case, the subtree from s is **pruned** to trade exploration for exploitation. In both cases, DFS backtracks to the parent state of s to continue exploration.

![](/paper_bites/tot_2.png#center)


## Build personal chatbot

The key items of building a personal chatbot is:

- Chat history
- user input
- LLM
- relevant data
- vector database

The key two steps are:

1. data ingestion
2. calling LLM with structured prompt


## Presentation of Andrej Karpathy on Microsoft Build 2023

Andrej Karpathy offered some thought-provoking insights into the current state of Large Language Models (LLM) during his speech at Microsoft Build 2023. I found several points particularly striking, and have summarized these key takeaways for reference.


### RLHF reduces the entropy of base model

Reinforcement Learning from Human Feedback (RLHF) can assist in lowering the entropy of the base model, thereby enabling it to produce a limited but persuasive set of outputs. Conversely, the base model could be inherently skilled at tasks requiring a diverse range of outputs.

![](/paper_bites/andrej_karpathy_ms_build_1.png#center)

### Why chain of thought works?

The GPT model utilizes a Transformer decoder, which generates the (n+1)th token based on the previous n tokens. To fully accomplish a task, the model requires a broader context, represented by more tokens, to extend its logical reasoning. Therefore, segmenting the task into smaller steps allows the model to distribute its reasoning process over a larger set of tokens.

![](/paper_bites/andrej_karpathy_ms_build_2.png#center)


### Ask the LLM for specifically what you want

LLM is just imitating the training data with a apectrum of performance qualities and they don't know what you really want. If you want specific thing, make it explicit. Like "think step by step to ensure we get the correct answer"

![](/paper_bites/andrej_karpathy_ms_build_3.png#center)


### Recommendations for current LLM race

Training a high-performance LLM is a costly undertaking, which leads many to consider leveraging an existing LLM API. However, companies often have privacy concerns and consider their data as a strategic asset, although this may not always be the case.

![](/paper_bites/andrej_karpathy_ms_build_4.png#center)

## LIMA: Less Is More for Alignment

RLHF play an important role aligning LLM with human's preferance. This paper suggests:
- almost all knowledge in large language models is learned during pretraining
- only limited instruction tuning data is necessary to teach models to produce high quality output

Therefore they propose **Superficial Alignment Hypothesis**:   
A model’s knowledge and capabilities are learnt almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users.

### Prompt source
To test this hypothesis, they curate 1,000 examples that approximate real user prompts and high-quality responses:
- select 750 top questions and answers from community forums, such as Stack Exchange and wikiHow, sampling for quality and diversity.  
- manually write 250 examples of prompts and responses, while optimizing for task diversity and emphasizing a uniform response style in the spirit of an AI assistant. 

### Training  

The LIMA is finetuned with 1000 demonstrations on a pretrained 65B-parameter LLaMa model.
To differentiate between each speaker (user and assistant), they introduce a special end-of-turn token (EOT) at the end of each utterance; this token plays the same role as EOS of halting generation, but avoids conflation with any other meaning that the pretrained model may have imbued into the preexisting EOS token.

Fine-tuning hyperparameters: 
- fine-tune for 15 epochs using AdamW with $\beta_1$ = 0.9, $\beta_1$ = 0.95;
- weight decay of 0.1;
- without warmup steps;
- the initial learning rate to 1$e$ − 5 and **linearly** decaying to 1$e$ − 6 by the end of training;
- batch size is set to 32 examples (64 for smaller models);
- texts longer than 2048 tokens are trimmed;
- One notable deviation from the norm is the use of residual dropout; apply dropout over residual connections, starting at $p_d$ = 0.0 **at the bottom layer** and linearly raising the rate to $p_d$ = 0.3 **at the last layer** ($p_d$ = 0.2 for smaller models). 

Despite claiming performance comparable to GPT-4, the paper's figures show that LIMA falls short in comparison to GPT-4 in **57%** of cases in human evaluations and **66%** of cases when GPT-4 acts as the annotator.

![](/paper_bites/LIMA_1.png#center)