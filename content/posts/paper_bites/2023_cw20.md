---
title: "Paper bites of 2023 CW20"
date: 2023-05-15T20:22:19+02:00
draft: false
mathjax: true
---

## FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance

FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. 

If the tasks are relatively simple, then aggregating multiple responses from GPT-J(30x smaller than GPT-3) offers performance similar to GPT-3

### Cost-saving strategies of using LLM

#### Prompt adaptation: decrease the prompt's size. 
- Prompt selection: rather than employing a prompt containing numerous examples that demonstrate how to perform a task, one can retain a small subset of examples in the prompt. 
![](/paper_bites/frugalgpt_1.png#center)
- Query concatenation: sending the prompt only once to the LLM API while allowing it to address multiple queries to prevent redundant prompt processing. To accomplish this, several queries must be concatenated into a single query, and the prompt must explicitly request the LLM API to process multiple queries.
![](/paper_bites/frugalgpt_2.png#center)

#### LLM approximation
- Completion cache: To process a new query, we first verify if a similar query has been previously answered. If so, the response is retrieved from the cache. An LLM API is invoked only if no similar query is discovered in the cache.
![](/paper_bites/frugalgpt_3.png#center)
- Model finetuning: 
    - 1. collect a powerful but expensive LLM API’s responses to a few queries; 
    - 2. use the responses to fine-tune a smaller and more affordable AI model; 
    - 3. employ the fine-tuned model for new queries. 
    In addition to cost savings, the fine-tuned model often does not require lengthy prompts, thus providing latency improvements as a byproduct.
![](/paper_bites/frugalgpt_4.png#center)
    

#### LLM cascade
- LLM cascade sends a query to a list of LLM APIs sequentially. If one LLM API’s response is reliable, then its response is returned, and no further LLMs in the list are needed. 
- The remaining LLM APIs are queried only if the previous APIs’ generations are deemed insufficiently reliable. 
- Query cost is significantly reduced if the first few APIs are relatively inexpensive and produce reliable generations.  

The key components of LLM cascade consist of two elements: 
    - 1. a generation scoring function (DistilBert is used in the paper) 
    - 2. an LLM router. 
    
Given a new query, it iteratively invokes the ith API in the list to obtain an answer $f_{Li}(q)$. 
Then, it uses the scoring function to generate a score $g(q, f_{Li}(q))$. It returns the generation if the score is higher than a threshold $\pi_{i}$, and queries the next service otherwise.  

The scoring function can be obtained by training a simple regression model that learns whether a generation is correct from the query and a generated answer.

![](/paper_bites/frugalgpt_5.png#center)

### Why multiple LLMs works better than individual powerfull LLM
Why can multiple LLM APIs potentially produce better performance than the best individual LLM? In essence, this is due to generation diversity: even an inexpensive LLM can sometimes correctly answer queries on which a more expensive LLM fails. To measure this diversity, we use the maximum performance improvement, or **MPI**.   
The **MPI** of `LLM A` with respect to `LLM B` is the probability that `LLM A` generates the correct answer while `LLM B` provides incorrect ones. This metric essentially measures the **maximum performance gains** achievable by invoking `LLM A` in addition to `LLM B`.


## Finetuning vs Embedding search

Some insights from [OpenAI Q&A: Finetuning GPT-3 vs Semantic Search - which to use, when, and why?](https://www.youtube.com/watch?v=9qq6HTr7Ocw)

When should applly finetuning?
- Teaching the model new patterns, specific formats, etc
- what's pattern:
    - email
    - json, html, xml, perl, c++

How to do QA with embedding search:
1. index your corpus with semantic embeddings which make the whole thing searchable
2. use LLM to come up with relevant search items, queries:
    - translate the queries to a seantic ebedding, sue it to search your corpus
    - pull most relevant search results
3. use LLM to pose your question against the relevant search results, compile all the answers together like notes

## TinyStories: How Small Can Language Models Be and Still Speak Coherent English?

Does the emergence of the ability to produce coherent English text only occur at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention)? The author released TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4.


### Generate diverse story datasets with LLM

The dataset rely on GPT-4 to produce content. To generate a diverse dataset with LLM is challenging even with a high temperature. To address the problem of diversity:  
- collected a vocabulary consisting of about 1500 basic words of a typical 3-4 year-old child's vocabulary
- separated the basic words into **nouns**, **verbs**, and **adjectives**. 
- in each generation, 3 words are chosen randomly (one verb, one noun, and one adjective). 
- the model is instructed to generate a story that somehow combines these random words into the story. 


### Evaluation with GPT-Eval

- provide the model with a story’s beginning, which is taken from a manually-prepared dataset consisting of around 50 prompts, 
- generate a completion using the model, 
- send (the story’s beginning, model’s completion) to GPT-4, asking it to grade the completion assignment in terms of **grammar**, **creativity**, and its **consistency** with the beginning of the story. 

To perform the full evaluation, for each of the manutally-constructed prompts in the evaluation set, 10 completions are generated from the trained model with **temperature=1**. The the evaluation scores of all completions  are averaged. 


### Some interesting findings
- shallower models perform better in terms of grammar compared to content consistency, meaning that model depth is more important for keeping consistent with the content than for generating syntactically correct language

- models that have only 1 layer seem to struggle with following instructions (which likely heavily relies on global attention), and 2 layers seem to be sufficient for a certain extent of instruction-following.

- as the embedding dimension and the number of layers increase, the performance improves. The models with higher embedding dimensions and more layers tend to generate more accurate, relevant, and natural continuations, while the models with lower embedding dimensions and fewer layers tend to generate more nonsensical, contradictory, or irrelevant continuations

- knowledge of facts seems to rely more on the embedding dimension, whereas for
context-tracking the number of layers is more important. For example, the model that has only 1 layer does not get any consistency prompt right, but does get some facts right, whereas the model with embedding dimension 64 does not get any fact right, but manages to maintain consistency several times. 

- when models have a small number of neurons and/or a small number of layers, we observe that both attention heads and MLP neurons have a meaningful function:   
    - Attention heads produce very clear attention patterns, with a clear separation between local and semantic heads, 
    - and MLP neurons typically activated on tokens that have a clear common role in the sentence.


### Model memorization

There are three levels of memorization:  

- Exact memorization: copies a an entire story or a large portion of it from the dataset, without changing anything. 

- Simple template matching: the model changes some names or entities in a story from the dataset, 

- Complex template matching: This is the most subtle and difficult form of memorization, where the model follows a more abstract pattern or structure from the dataset, keeping the general plot but changing the details and the specifics of the story.

